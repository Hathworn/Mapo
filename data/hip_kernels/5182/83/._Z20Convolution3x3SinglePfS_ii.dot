digraph "CFG for '_Z20Convolution3x3SinglePfS_ii' function" {
	label="CFG for '_Z20Convolution3x3SinglePfS_ii' function";

	Node0x648ea60 [shape=record,color="#b70d28ff", style=filled, fillcolor="#b70d2870",label="{%4:\l  %5 = tail call align 4 dereferenceable(64) i8 addrspace(4)*\l... @llvm.amdgcn.dispatch.ptr()\l  %6 = getelementptr i8, i8 addrspace(4)* %5, i64 4\l  %7 = bitcast i8 addrspace(4)* %6 to i16 addrspace(4)*\l  %8 = load i16, i16 addrspace(4)* %7, align 4, !range !4, !invariant.load !5\l  %9 = zext i16 %8 to i32\l  %10 = getelementptr inbounds i8, i8 addrspace(4)* %5, i64 12\l  %11 = bitcast i8 addrspace(4)* %10 to i32 addrspace(4)*\l  %12 = load i32, i32 addrspace(4)* %11, align 4, !tbaa !6\l  %13 = tail call i32 @llvm.amdgcn.workgroup.id.x()\l  %14 = tail call i32 @llvm.amdgcn.workgroup.id.y()\l  %15 = udiv i32 %12, %9\l  %16 = mul i32 %15, %9\l  %17 = icmp ugt i32 %12, %16\l  %18 = zext i1 %17 to i32\l  %19 = add i32 %15, %18\l  %20 = mul i32 %19, %14\l  %21 = tail call i32 @llvm.amdgcn.workitem.id.x(), !range !15\l  %22 = add i32 %20, %13\l  %23 = mul i32 %22, %9\l  %24 = add i32 %23, %21\l  %25 = mul nsw i32 %3, %2\l  %26 = icmp slt i32 %24, %25\l  br i1 %26, label %27, label %121\l|{<s0>T|<s1>F}}"];
	Node0x648ea60:s0 -> Node0x6490d30;
	Node0x648ea60:s1 -> Node0x6490dc0;
	Node0x6490d30 [shape=record,color="#3d50c3ff", style=filled, fillcolor="#f59c7d70",label="{%27:\l27:                                               \l  %28 = freeze i32 %24\l  %29 = freeze i32 %2\l  %30 = sdiv i32 %28, %29\l  %31 = mul i32 %30, %29\l  %32 = sub i32 %28, %31\l  %33 = tail call i32 @llvm.smax.i32(i32 %32, i32 1)\l  %34 = add nsw i32 %33, -1\l  %35 = add nsw i32 %2, -1\l  %36 = tail call i32 @llvm.smin.i32(i32 %34, i32 %35)\l  %37 = tail call i32 @llvm.smax.i32(i32 %30, i32 1)\l  %38 = add nsw i32 %37, -1\l  %39 = add nsw i32 %3, -1\l  %40 = tail call i32 @llvm.smin.i32(i32 %38, i32 %39)\l  %41 = load float, float addrspace(4)* getelementptr inbounds ([9 x float],\l... [9 x float] addrspace(4)* @D_KERNEL, i64 0, i64 0), align 16, !tbaa !16\l  %42 = mul nsw i32 %40, %2\l  %43 = add nsw i32 %42, %36\l  %44 = sext i32 %43 to i64\l  %45 = getelementptr inbounds float, float addrspace(1)* %0, i64 %44\l  %46 = load float, float addrspace(1)* %45, align 4, !tbaa !16,\l... !amdgpu.noclobber !5\l  %47 = fmul contract float %41, %46\l  %48 = fadd contract float %47, 0.000000e+00\l  %49 = tail call i32 @llvm.smax.i32(i32 %30, i32 0)\l  %50 = tail call i32 @llvm.smin.i32(i32 %49, i32 %39)\l  %51 = load float, float addrspace(4)* getelementptr inbounds ([9 x float],\l... [9 x float] addrspace(4)* @D_KERNEL, i64 0, i64 3), align 4, !tbaa !16\l  %52 = mul nsw i32 %50, %2\l  %53 = add nsw i32 %52, %36\l  %54 = sext i32 %53 to i64\l  %55 = getelementptr inbounds float, float addrspace(1)* %0, i64 %54\l  %56 = load float, float addrspace(1)* %55, align 4, !tbaa !16,\l... !amdgpu.noclobber !5\l  %57 = fmul contract float %51, %56\l  %58 = fadd contract float %48, %57\l  %59 = tail call i32 @llvm.smax.i32(i32 %30, i32 -1)\l  %60 = add nsw i32 %59, 1\l  %61 = tail call i32 @llvm.smin.i32(i32 %60, i32 %39)\l  %62 = load float, float addrspace(4)* getelementptr inbounds ([9 x float],\l... [9 x float] addrspace(4)* @D_KERNEL, i64 0, i64 6), align 8, !tbaa !16\l  %63 = mul nsw i32 %61, %2\l  %64 = add nsw i32 %63, %36\l  %65 = sext i32 %64 to i64\l  %66 = getelementptr inbounds float, float addrspace(1)* %0, i64 %65\l  %67 = load float, float addrspace(1)* %66, align 4, !tbaa !16,\l... !amdgpu.noclobber !5\l  %68 = fmul contract float %62, %67\l  %69 = fadd contract float %58, %68\l  %70 = tail call i32 @llvm.smax.i32(i32 %32, i32 0)\l  %71 = tail call i32 @llvm.smin.i32(i32 %70, i32 %35)\l  %72 = load float, float addrspace(4)* getelementptr inbounds ([9 x float],\l... [9 x float] addrspace(4)* @D_KERNEL, i64 0, i64 1), align 4, !tbaa !16\l  %73 = add nsw i32 %42, %71\l  %74 = sext i32 %73 to i64\l  %75 = getelementptr inbounds float, float addrspace(1)* %0, i64 %74\l  %76 = load float, float addrspace(1)* %75, align 4, !tbaa !16,\l... !amdgpu.noclobber !5\l  %77 = fmul contract float %72, %76\l  %78 = fadd contract float %69, %77\l  %79 = load float, float addrspace(4)* getelementptr inbounds ([9 x float],\l... [9 x float] addrspace(4)* @D_KERNEL, i64 0, i64 4), align 16, !tbaa !16\l  %80 = add nsw i32 %52, %71\l  %81 = sext i32 %80 to i64\l  %82 = getelementptr inbounds float, float addrspace(1)* %0, i64 %81\l  %83 = load float, float addrspace(1)* %82, align 4, !tbaa !16,\l... !amdgpu.noclobber !5\l  %84 = fmul contract float %79, %83\l  %85 = fadd contract float %78, %84\l  %86 = load float, float addrspace(4)* getelementptr inbounds ([9 x float],\l... [9 x float] addrspace(4)* @D_KERNEL, i64 0, i64 7), align 4, !tbaa !16\l  %87 = add nsw i32 %63, %71\l  %88 = sext i32 %87 to i64\l  %89 = getelementptr inbounds float, float addrspace(1)* %0, i64 %88\l  %90 = load float, float addrspace(1)* %89, align 4, !tbaa !16,\l... !amdgpu.noclobber !5\l  %91 = fmul contract float %86, %90\l  %92 = fadd contract float %85, %91\l  %93 = tail call i32 @llvm.smax.i32(i32 %32, i32 -1)\l  %94 = add nsw i32 %93, 1\l  %95 = tail call i32 @llvm.smin.i32(i32 %94, i32 %35)\l  %96 = load float, float addrspace(4)* getelementptr inbounds ([9 x float],\l... [9 x float] addrspace(4)* @D_KERNEL, i64 0, i64 2), align 8, !tbaa !16\l  %97 = add nsw i32 %42, %95\l  %98 = sext i32 %97 to i64\l  %99 = getelementptr inbounds float, float addrspace(1)* %0, i64 %98\l  %100 = load float, float addrspace(1)* %99, align 4, !tbaa !16,\l... !amdgpu.noclobber !5\l  %101 = fmul contract float %96, %100\l  %102 = fadd contract float %92, %101\l  %103 = load float, float addrspace(4)* getelementptr inbounds ([9 x float],\l... [9 x float] addrspace(4)* @D_KERNEL, i64 0, i64 5), align 4, !tbaa !16\l  %104 = add nsw i32 %52, %95\l  %105 = sext i32 %104 to i64\l  %106 = getelementptr inbounds float, float addrspace(1)* %0, i64 %105\l  %107 = load float, float addrspace(1)* %106, align 4, !tbaa !16,\l... !amdgpu.noclobber !5\l  %108 = fmul contract float %103, %107\l  %109 = fadd contract float %102, %108\l  %110 = load float, float addrspace(4)* getelementptr inbounds ([9 x float],\l... [9 x float] addrspace(4)* @D_KERNEL, i64 0, i64 8), align 16, !tbaa !16\l  %111 = add nsw i32 %63, %95\l  %112 = sext i32 %111 to i64\l  %113 = getelementptr inbounds float, float addrspace(1)* %0, i64 %112\l  %114 = load float, float addrspace(1)* %113, align 4, !tbaa !16,\l... !amdgpu.noclobber !5\l  %115 = fmul contract float %110, %114\l  %116 = fadd contract float %109, %115\l  %117 = mul nsw i32 %30, %2\l  %118 = add nsw i32 %117, %32\l  %119 = sext i32 %118 to i64\l  %120 = getelementptr inbounds float, float addrspace(1)* %1, i64 %119\l  store float %116, float addrspace(1)* %120, align 4, !tbaa !16\l  br label %121\l}"];
	Node0x6490d30 -> Node0x6490dc0;
	Node0x6490dc0 [shape=record,color="#b70d28ff", style=filled, fillcolor="#b70d2870",label="{%121:\l121:                                              \l  ret void\l}"];
}
